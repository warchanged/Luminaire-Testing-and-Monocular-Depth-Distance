"""
ä¼˜åŒ–ç‰ˆGradio Web UI - ç¯å…·3Då®šä½å®æ—¶æ£€æµ‹
1. TensorRTåŠ é€Ÿæ¨ç†
2. é—´éš”é‡‡æ ·æ£€æµ‹(æ¯Nç§’æ£€æµ‹ä¸€å¸§)
"""

import gradio as gr
import cv2
import numpy as np
from PIL import Image, ImageDraw, ImageFont
import torch
from pipeline import LightLocalization3D
import time
import threading
import queue

# å…¨å±€å˜é‡
pipeline = None
last_detection_time = 0
detection_interval = 10  # æ¯10ç§’æ£€æµ‹ä¸€æ¬¡
processing_queue = queue.Queue(maxsize=1)

def initialize_pipeline():
    """åˆå§‹åŒ–æ£€æµ‹æµæ°´çº¿"""
    global pipeline
    if pipeline is None:
        print("ğŸš€ åˆå§‹åŒ–ç¯å…·æ£€æµ‹æµæ°´çº¿...")
        pipeline = LightLocalization3D(
            detection_model="google/owlv2-large-patch14-ensemble",
            feature_model="facebook/dinov2-large",
            depth_model="depth-anything/Depth-Anything-V2-Large-hf"
        )
        
        # å°è¯•å¯ç”¨TensorRTåŠ é€Ÿ
        try:
            if hasattr(pipeline, 'enable_tensorrt'):
                pipeline.enable_tensorrt()
                print("âœ… TensorRTåŠ é€Ÿå·²å¯ç”¨")
        except Exception as e:
            print(f"âš ï¸ TensorRTåŠ é€Ÿå¯ç”¨å¤±è´¥: {e}")
        
        print("âœ… æµæ°´çº¿åˆå§‹åŒ–å®Œæˆ!")
    return pipeline

def draw_detections(image, detections):
    """åœ¨å›¾åƒä¸Šç»˜åˆ¶æ£€æµ‹ç»“æœ"""
    if isinstance(image, np.ndarray):
        image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
    
    draw = ImageDraw.Draw(image)
    
    try:
        font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 16)
        font_small = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 12)
    except:
        font = ImageFont.load_default()
        font_small = font
    
    colors = [
        (255, 0, 0), (0, 255, 0), (0, 0, 255),
        (255, 255, 0), (255, 0, 255), (0, 255, 255)
    ]
    
    for idx, det in enumerate(detections):
        box = det['box']
        x1, y1, x2, y2 = map(int, box)
        color = colors[idx % len(colors)]
        
        draw.rectangle([x1, y1, x2, y2], outline=color, width=3)
        
        label = det['label']
        confidence = det['confidence']
        distance = det.get('distance', None)
        
        if distance:
            text = f"{label}\n{confidence:.1%}\n{distance:.2f}m"
        else:
            text = f"{label}\n{confidence:.1%}"
        
        bbox = draw.textbbox((x1, y1-60), text, font=font_small)
        draw.rectangle([bbox[0]-2, bbox[1]-2, bbox[2]+2, bbox[3]+2], fill=(0, 0, 0, 200))
        draw.text((x1, y1-60), text, fill=color, font=font_small)
    
    return np.array(image)

def process_image(image, confidence_threshold, show_depth):
    """å¤„ç†å•å¼ å›¾åƒ"""
    try:
        if image is None:
            return None, None, "âŒ è¯·å…ˆä¸Šä¼ å›¾ç‰‡!"
        
        start_time = time.time()
        pipe = initialize_pipeline()
        
        if isinstance(image, Image.Image):
            image = np.array(image)
        
        if len(image.shape) == 2:
            image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)
        elif image.shape[2] == 4:
            image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)
        
        print(f"å¤„ç†å›¾åƒ: shape={image.shape}, threshold={confidence_threshold}")
        
        # æ‰§è¡Œæ£€æµ‹
        result = pipe.process_image(
            image,
            confidence_threshold=confidence_threshold,
            compute_depth=show_depth,
            compute_distance=show_depth
        )
        
        detections = result['detections']
        depth_map = result.get('depth_map')
        
        output_image = draw_detections(image.copy(), detections)
        process_time = time.time() - start_time
        fps = 1.0 / result['timing']['total']
        
        stats = f"""
        ### ğŸ“Š æ£€æµ‹ç»Ÿè®¡
        - **æ£€æµ‹æ•°é‡**: {len(detections)} ä¸ªç¯å…·
        - **å¤„ç†æ—¶é—´**: {process_time:.2f}ç§’
        - **FPS**: {fps:.2f}
        - **ç½®ä¿¡åº¦é˜ˆå€¼**: {confidence_threshold:.2f}
        """
        
        details = "### ğŸ” æ£€æµ‹è¯¦æƒ…\n\n"
        for i, det in enumerate(detections[:10], 1):
            details += f"**ç›®æ ‡ {i}**\n"
            details += f"- ç±»å‹: {det['label']}\n"
            details += f"- ç½®ä¿¡åº¦: {det['confidence']:.2%}\n"
            if det.get('distance'):
                details += f"- è·ç¦»: {det['distance']:.2f}m\n"
            details += "\n"
        
        depth_image = None
        if show_depth and depth_map is not None:
            import matplotlib.pyplot as plt
            import matplotlib
            matplotlib.use('Agg')
            
            fig, ax = plt.subplots(figsize=(8, 6))
            im = ax.imshow(depth_map, cmap='plasma')
            ax.set_title('Depth Map', fontsize=14)  # è‹±æ–‡é¿å…ä¸­æ–‡å­—ä½“é—®é¢˜
            ax.axis('off')
            plt.colorbar(im, ax=ax, label='Depth (normalized)')
            
            fig.canvas.draw()
            buf = fig.canvas.buffer_rgba()
            depth_image = np.asarray(buf)[:, :, :3]
            plt.close(fig)
        
        return output_image, depth_image, stats + "\n" + details
        
    except Exception as e:
        import traceback
        error_msg = f"âŒ å¤„ç†å¤±è´¥: {str(e)}\n\n```\n{traceback.format_exc()}\n```"
        return image, None, error_msg

def process_frame_interval(frame, confidence_threshold, interval_seconds):
    """é—´éš”é‡‡æ ·å¤„ç†è§†é¢‘å¸§"""
    global last_detection_time
    
    current_time = time.time()
    time_since_last = current_time - last_detection_time
    
    # åœ¨å›¾åƒä¸Šæ˜¾ç¤ºå€’è®¡æ—¶
    output_frame = frame.copy()
    
    if time_since_last >= interval_seconds:
        # æ‰§è¡Œæ£€æµ‹
        try:
            pipe = initialize_pipeline()
            
            result = pipe.process_image(
                frame,
                confidence_threshold=confidence_threshold,
                compute_depth=False,
                compute_distance=False
            )
            
            output_frame = draw_detections(frame, result['detections'])
            last_detection_time = current_time
            
            # æ˜¾ç¤ºæ£€æµ‹ä¿¡æ¯
            info_text = f"Detected: {len(result['detections'])} lights"
            cv2.putText(output_frame, info_text, (10, 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
            cv2.putText(output_frame, f"Next in: {interval_seconds}s", (10, 70),
                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        except Exception as e:
            print(f"æ£€æµ‹é”™è¯¯: {e}")
            cv2.putText(output_frame, f"Error: {str(e)[:50]}", (10, 30),
                       cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
    else:
        # æ˜¾ç¤ºå€’è®¡æ—¶
        remaining = int(interval_seconds - time_since_last)
        cv2.putText(output_frame, f"Next detection in: {remaining}s", (10, 30),
                   cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)
        cv2.putText(output_frame, "Waiting...", (10, 70),
                   cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), 2)
    
    return output_frame

# åˆ›å»ºGradioç•Œé¢
with gr.Blocks(title="ç¯å…·3Då®šä½æ£€æµ‹ç³»ç»Ÿ (ä¼˜åŒ–ç‰ˆ)", theme=gr.themes.Soft()) as demo:
    gr.Markdown("""
    # ğŸ”¦ ç¯å…·3Då®šä½æ£€æµ‹ç³»ç»Ÿ (TensorRTä¼˜åŒ–ç‰ˆ)
    
    åŸºäº **OWLv2 + DINOv3 + Depth Anything V2** çš„æ™ºèƒ½ç¯å…·æ£€æµ‹ä¸3Då®šä½
    
    âš¡ **æ€§èƒ½ä¼˜åŒ–**: TensorRTåŠ é€Ÿ | é—´éš”é‡‡æ · | GPUåŠ é€Ÿ
    """)
    
    with gr.Tabs():
        # Tab 1: å›¾åƒä¸Šä¼ æ£€æµ‹
        with gr.Tab("ğŸ“¸ å›¾åƒæ£€æµ‹"):
            with gr.Row():
                with gr.Column():
                    input_image = gr.Image(label="ä¸Šä¼ å›¾åƒ", type="numpy")
                    confidence_slider = gr.Slider(
                        minimum=0.05,
                        maximum=0.5,
                        value=0.15,
                        step=0.05,
                        label="ç½®ä¿¡åº¦é˜ˆå€¼"
                    )
                    show_depth_check = gr.Checkbox(
                        label="æ˜¾ç¤ºæ·±åº¦å›¾",
                        value=True
                    )
                    detect_btn = gr.Button("ğŸ” å¼€å§‹æ£€æµ‹", variant="primary")
                
                with gr.Column():
                    output_image = gr.Image(label="æ£€æµ‹ç»“æœ")
                    depth_image = gr.Image(label="æ·±åº¦å›¾", visible=True)
            
            with gr.Row():
                stats_output = gr.Markdown(label="æ£€æµ‹ç»Ÿè®¡")
            
            detect_btn.click(
                fn=process_image,
                inputs=[input_image, confidence_slider, show_depth_check],
                outputs=[output_image, depth_image, stats_output]
            )
        
        # Tab 2: é—´éš”é‡‡æ ·æ£€æµ‹
        with gr.Tab("ğŸ¥ é—´éš”æ£€æµ‹ (ä¼˜åŒ–)"):
            gr.Markdown("""
            ### âš¡ ä¼˜åŒ–è¯´æ˜
            
            **æ€§èƒ½ä¼˜åŒ–**:
            - âœ… æ¯Nç§’æ£€æµ‹ä¸€å¸§(é»˜è®¤10ç§’)
            - âœ… å¤§å¹…é™ä½GPUè´Ÿè½½
            - âœ… é¿å…è¿ç»­æ¨ç†å¯¼è‡´çš„å»¶è¿Ÿ
            - âœ… é€‚åˆé•¿æ—¶é—´ç›‘æ§åœºæ™¯
            
            **ä½¿ç”¨æ–¹æ³•**:
            1. ä¸Šä¼ è§†é¢‘å¸§æˆ–å›¾ç‰‡
            2. è°ƒæ•´æ£€æµ‹é—´éš”(ç§’)
            3. è°ƒæ•´ç½®ä¿¡åº¦é˜ˆå€¼
            4. ç‚¹å‡»"å¼€å§‹æ£€æµ‹"
            """)
            
            with gr.Row():
                with gr.Column():
                    video_input = gr.Image(label="ä¸Šä¼ å›¾ç‰‡", type="numpy")
                    interval_slider = gr.Slider(
                        minimum=1,
                        maximum=30,
                        value=10,
                        step=1,
                        label="æ£€æµ‹é—´éš”(ç§’)"
                    )
                    video_confidence = gr.Slider(
                        minimum=0.05,
                        maximum=0.5,
                        value=0.15,
                        step=0.05,
                        label="ç½®ä¿¡åº¦é˜ˆå€¼"
                    )
                    process_btn = gr.Button("ğŸ” å¼€å§‹æ£€æµ‹", variant="primary")
                
                with gr.Column():
                    video_output = gr.Image(label="æ£€æµ‹ç»“æœ")
                    video_stats = gr.Markdown(label="ç»Ÿè®¡ä¿¡æ¯")
            
            process_btn.click(
                fn=lambda img, conf, interval: (
                    process_frame_interval(img, conf, interval),
                    f"**æ£€æµ‹é—´éš”**: {interval}ç§’\n**ç½®ä¿¡åº¦**: {conf:.2f}"
                ),
                inputs=[video_input, video_confidence, interval_slider],
                outputs=[video_output, video_stats]
            )
        
        # Tab 3: ä½¿ç”¨è¯´æ˜
        with gr.Tab("ğŸ“¹ å®æ—¶æ£€æµ‹æŒ‡å—"):
            gr.Markdown("""
            # ğŸ¥ å®æ—¶æ‘„åƒå¤´æ£€æµ‹æ–¹æ¡ˆ
            
            ## æ¨èæ–¹æ¡ˆ: ä½¿ç”¨ webcam_client.html
            
            ### ä¸ºä»€ä¹ˆGradioä¸ç›´æ¥æ”¯æŒå®æ—¶æ‘„åƒå¤´?
            
            1. **æ€§èƒ½è€ƒè™‘**: è¿ç»­æ¨ç†ä¼šå¯¼è‡´GPUè¿‡è½½
            2. **å»¶è¿Ÿé—®é¢˜**: ç½‘ç»œä¼ è¾“ + æ¨ç†å»¶è¿Ÿ > 1ç§’
            3. **èµ„æºæ¶ˆè€—**: æŒç»­å ç”¨GPUèµ„æº
            
            ### è§£å†³æ–¹æ¡ˆ: æœ¬åœ°å®¢æˆ·ç«¯ + é—´éš”é‡‡æ ·
            
            **æ­¥éª¤**:
            
            1. æ‰“å¼€æœ¬åœ°çš„ `webcam_client.html`
            2. è¿æ¥åˆ°æœåŠ¡å™¨: `http://æœåŠ¡å™¨IP:7860`
            3. å¯åŠ¨æ‘„åƒå¤´
            4. è®¾ç½®é‡‡æ ·é—´éš”(å¦‚10ç§’æ£€æµ‹ä¸€æ¬¡)
            
            **ä¼˜åŠ¿**:
            - âš¡ æœ¬åœ°æ‘„åƒå¤´æ•è·(æ— å»¶è¿Ÿ)
            - ğŸ”„ é—´éš”å‘é€åˆ°æœåŠ¡å™¨(é™ä½è´Ÿè½½)
            - ğŸ“Š å®æ—¶æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            - ğŸ’¾ èŠ‚çœGPUèµ„æº
            
            ---
            
            ## é«˜çº§æ–¹æ¡ˆ: Pythonè„šæœ¬
            
            ```python
            # realtime_interval.py
            import cv2
            import time
            from pipeline import LightLocalization3D
            
            pipeline = LightLocalization3D(...)
            cap = cv2.VideoCapture(0)
            
            interval = 10  # æ¯10ç§’æ£€æµ‹ä¸€æ¬¡
            last_time = 0
            
            while True:
                ret, frame = cap.read()
                current_time = time.time()
                
                if current_time - last_time >= interval:
                    result = pipeline.process_image(frame)
                    last_time = current_time
                    # æ˜¾ç¤ºç»“æœ...
                
                cv2.imshow('Frame', frame)
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break
            ```
            
            ---
            
            ## TensorRTåŠ é€Ÿè¯´æ˜
            
            ### è‡ªåŠ¨å¯ç”¨
            
            ç³»ç»Ÿå·²è‡ªåŠ¨å°è¯•å¯ç”¨TensorRTåŠ é€Ÿã€‚
            
            ### é¢„æœŸæå‡
            
            | ä¼˜åŒ– | åŠ é€Ÿæ¯” | å¤‡æ³¨ |
            |------|--------|------|
            | TensorRT | 2-3x | é¦–æ¬¡è¿è¡Œéœ€ç¼–è¯‘ |
            | é—´éš”é‡‡æ · | 10x+ | é™ä½å¹³å‡è´Ÿè½½ |
            | FP16æ¨ç† | 1.5x | ç•¥å¾®é™ä½ç²¾åº¦ |
            
            ### éªŒè¯åŠ é€Ÿ
            
            æŸ¥çœ‹å¯åŠ¨æ—¥å¿—:
            - âœ… TensorRTåŠ é€Ÿå·²å¯ç”¨
            - âš ï¸ TensorRTåŠ é€Ÿå¯ç”¨å¤±è´¥
            
            ---
            
            **ä¸‹è½½**: `webcam_client.html` åœ¨é¡¹ç›®æ ¹ç›®å½•
            """)
        
        # Tab 4: ç³»ç»Ÿä¿¡æ¯
        with gr.Tab("â„¹ï¸ ç³»ç»Ÿä¿¡æ¯"):
            gr.Markdown("""
            ### æŠ€æœ¯æ¶æ„
            
            | ç»„ä»¶ | æ¨¡å‹ | ç”¨é€” |
            |------|------|------|
            | **æ£€æµ‹å™¨** | OWLv2-Large | é›¶æ ·æœ¬ç›®æ ‡æ£€æµ‹ |
            | **ç‰¹å¾æå–** | DINOv3-Large | è‡ªç›‘ç£è§†è§‰ç‰¹å¾ |
            | **æ·±åº¦ä¼°è®¡** | Depth Anything V2 | å•ç›®æ·±åº¦ä¼°è®¡ |
            | **åŠ é€Ÿ** | TensorRT + CUDA | æ¨ç†åŠ é€Ÿ |
            
            ### æ€§èƒ½ä¼˜åŒ–
            
            - âš¡ **TensorRTåŠ é€Ÿ**: 2-3å€é€Ÿåº¦æå‡
            - ğŸ”„ **é—´éš”é‡‡æ ·**: é™ä½90%+ GPUè´Ÿè½½
            - ğŸ“‰ **FP16æ¨ç†**: é™ä½æ˜¾å­˜å ç”¨
            - ğŸ¯ **æŒ‰éœ€è®¡ç®—**: ä»…åœ¨éœ€è¦æ—¶è®¡ç®—æ·±åº¦
            
            ### æ”¯æŒçš„ç¯å…·ç±»å‹
            
            - ğŸ® åŠç¯ç±»: chandelier, pendant light, hanging lamp
            - ğŸ’¡ å¸é¡¶ç¯: ceiling light, flush mount, recessed light
            - ğŸ•¯ï¸ å£ç¯: wall lamp, wall sconce
            - ğŸ›‹ï¸ å°ç¯: table lamp, desk lamp
            - ğŸŒŸ å°„ç¯: spotlight, track light
            - âœ¨ LEDç¯: LED panel, LED strip
            - ... å…±33ç§ç±»å‹
            
            ### GitHub
            
            [ğŸ”— é¡¹ç›®åœ°å€](https://github.com/warchanged/Luminaire-Testing-and-Monocular-Depth-Distance)
            """)

if __name__ == "__main__":
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True
    )
