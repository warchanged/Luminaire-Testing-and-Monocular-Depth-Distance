"""
ä¼˜åŒ–ç‰ˆGradio Web UI - ç¯å…·3Då®šä½å®æ—¶æ£€æµ‹
1. TensorRTåŠ é€Ÿæ¨ç†
2. é—´éš”é‡‡æ ·æ£€æµ‹(æ¯Nç§’æ£€æµ‹ä¸€å¸§)
"""

import gradio as gr
import cv2
import numpy as np
from PIL import Image, ImageDraw, ImageFont
import torch
from pipeline import LightLocalization3D
import time
import threading
import queue

# å…¨å±€å˜é‡
pipeline = None
last_detection_time = 0
detection_interval = 10  # æ¯10ç§’æ£€æµ‹ä¸€æ¬¡
processing_queue = queue.Queue(maxsize=1)

def initialize_pipeline():
    """åˆå§‹åŒ–æ£€æµ‹æµæ°´çº¿"""
    global pipeline
    if pipeline is None:
        print("ğŸš€ åˆå§‹åŒ–ç¯å…·æ£€æµ‹æµæ°´çº¿...")
        pipeline = LightLocalization3D(
            detection_model="google/owlv2-large-patch14-ensemble",
            feature_model="facebook/dinov2-large",
            depth_model="depth-anything/Depth-Anything-V2-Large-hf"
        )
        
        # å°è¯•å¯ç”¨TensorRTåŠ é€Ÿ
        try:
            if hasattr(pipeline, 'enable_tensorrt'):
                pipeline.enable_tensorrt()
                print("âœ… TensorRTåŠ é€Ÿå·²å¯ç”¨")
        except Exception as e:
            print(f"âš ï¸ TensorRTåŠ é€Ÿå¯ç”¨å¤±è´¥: {e}")
        
        print("âœ… æµæ°´çº¿åˆå§‹åŒ–å®Œæˆ!")
    return pipeline

def draw_detections(image, detections):
    """åœ¨å›¾åƒä¸Šç»˜åˆ¶æ£€æµ‹ç»“æœ"""
    if isinstance(image, np.ndarray):
        image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
    
    draw = ImageDraw.Draw(image)
    
    try:
        font = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf", 16)
        font_small = ImageFont.truetype("/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf", 12)
    except:
        font = ImageFont.load_default()
        font_small = font
    
    colors = [
        (255, 0, 0), (0, 255, 0), (0, 0, 255),
        (255, 255, 0), (255, 0, 255), (0, 255, 255)
    ]
    
    for idx, det in enumerate(detections):
        box = det['box']
        x1, y1, x2, y2 = map(int, box)
        color = colors[idx % len(colors)]
        
        draw.rectangle([x1, y1, x2, y2], outline=color, width=3)
        
        label = det['label']
        confidence = det['confidence']
        distance = det.get('distance', None)
        
        if distance:
            text = f"{label}\n{confidence:.1%}\n{distance:.2f}m"
        else:
            text = f"{label}\n{confidence:.1%}"
        
        bbox = draw.textbbox((x1, y1-60), text, font=font_small)
        draw.rectangle([bbox[0]-2, bbox[1]-2, bbox[2]+2, bbox[3]+2], fill=(0, 0, 0, 200))
        draw.text((x1, y1-60), text, fill=color, font=font_small)
    
    return np.array(image)

def process_image(image, confidence_threshold, show_depth):
    """å¤„ç†å•å¼ å›¾åƒ"""
    try:
        if image is None:
            return None, None, "âŒ è¯·å…ˆä¸Šä¼ å›¾ç‰‡!"
        
        start_time = time.time()
        pipe = initialize_pipeline()
        
        if isinstance(image, Image.Image):
            image = np.array(image)
        
        if len(image.shape) == 2:
            image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)
        elif image.shape[2] == 4:
            image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)
        
        print(f"å¤„ç†å›¾åƒ: shape={image.shape}, threshold={confidence_threshold}")
        
        # æ‰§è¡Œæ£€æµ‹
        result = pipe.process_image(
            image,
            confidence_threshold=confidence_threshold,
            compute_depth=show_depth,
            compute_distance=show_depth
        )
        
        detections = result['detections']
        depth_map = result.get('depth_map')
        
        output_image = draw_detections(image.copy(), detections)
        process_time = time.time() - start_time
        fps = 1.0 / result['timing']['total']
        
        stats = f"""
        ### ğŸ“Š æ£€æµ‹ç»Ÿè®¡
        - **æ£€æµ‹æ•°é‡**: {len(detections)} ä¸ªç¯å…·
        - **å¤„ç†æ—¶é—´**: {process_time:.2f}ç§’
        - **FPS**: {fps:.2f}
        - **ç½®ä¿¡åº¦é˜ˆå€¼**: {confidence_threshold:.2f}
        """
        
        details = "### ğŸ” æ£€æµ‹è¯¦æƒ…\n\n"
        for i, det in enumerate(detections[:10], 1):
            details += f"**ç›®æ ‡ {i}**\n"
            details += f"- ç±»å‹: {det['label']}\n"
            details += f"- ç½®ä¿¡åº¦: {det['confidence']:.2%}\n"
            if det.get('distance'):
                details += f"- è·ç¦»: {det['distance']:.2f}m\n"
            details += "\n"
        
        depth_image = None
        if show_depth and depth_map is not None:
            import matplotlib.pyplot as plt
            import matplotlib
            matplotlib.use('Agg')
            
            fig, ax = plt.subplots(figsize=(8, 6))
            im = ax.imshow(depth_map, cmap='plasma')
            ax.set_title('Depth Map', fontsize=14)  # è‹±æ–‡é¿å…ä¸­æ–‡å­—ä½“é—®é¢˜
            ax.axis('off')
            plt.colorbar(im, ax=ax, label='Depth (normalized)')
            
            fig.canvas.draw()
            buf = fig.canvas.buffer_rgba()
            depth_image = np.asarray(buf)[:, :, :3]
            plt.close(fig)
        
        return output_image, depth_image, stats + "\n" + details
        
    except Exception as e:
        import traceback
        error_msg = f"âŒ å¤„ç†å¤±è´¥: {str(e)}\n\n```\n{traceback.format_exc()}\n```"
        return image, None, error_msg

def process_frame_interval(frame, confidence_threshold, interval_seconds, show_depth):
    """é—´éš”é‡‡æ ·å¤„ç†è§†é¢‘å¸§ - å®Œæ•´åŠŸèƒ½ç‰ˆæœ¬"""
    global last_detection_time
    
    if frame is None:
        return None, None, "âŒ è¯·å…ˆä¸Šä¼ å›¾ç‰‡!"
    
    try:
        current_time = time.time()
        time_since_last = current_time - last_detection_time
        
        # æ‰§è¡Œå®Œæ•´æ£€æµ‹(åŒ…å«è·ç¦»)
        pipe = initialize_pipeline()
        
        result = pipe.process_image(
            frame,
            confidence_threshold=confidence_threshold,
            compute_depth=show_depth,
            compute_distance=True  # å¯ç”¨è·ç¦»æ£€æµ‹
        )
        
        detections = result['detections']
        depth_map = result.get('depth_map')
        
        output_frame = draw_detections(frame.copy(), detections)
        last_detection_time = current_time
        
        # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        stats = f"""
        ### ğŸ“Š æ£€æµ‹ç»Ÿè®¡
        - **æ£€æµ‹æ•°é‡**: {len(detections)} ä¸ªç¯å…·
        - **ç½®ä¿¡åº¦é˜ˆå€¼**: {confidence_threshold:.2f}
        - **æ£€æµ‹é—´éš”**: {interval_seconds}ç§’
        - **ä¸‹æ¬¡æ£€æµ‹**: {interval_seconds}ç§’å
        
        ### ğŸ” æ£€æµ‹è¯¦æƒ…
        """
        
        for i, det in enumerate(detections[:10], 1):
            stats += f"\n**ç›®æ ‡ {i}**\n"
            stats += f"- ç±»å‹: {det['label']}\n"
            stats += f"- ç½®ä¿¡åº¦: {det['confidence']:.2%}\n"
            if det.get('distance'):
                stats += f"- è·ç¦»: {det['distance']:.2f}m\n"
        
        # ç”Ÿæˆæ·±åº¦å›¾
        depth_image = None
        if show_depth and depth_map is not None:
            import matplotlib.pyplot as plt
            import matplotlib
            matplotlib.use('Agg')
            
            fig, ax = plt.subplots(figsize=(8, 6))
            im = ax.imshow(depth_map, cmap='plasma')
            ax.set_title('Depth Map', fontsize=14)
            ax.axis('off')
            plt.colorbar(im, ax=ax, label='Depth (normalized)')
            
            fig.canvas.draw()
            buf = fig.canvas.buffer_rgba()
            depth_image = np.asarray(buf)[:, :, :3]
            plt.close(fig)
        
        return output_frame, depth_image, stats
        
    except Exception as e:
        import traceback
        error_msg = f"âŒ å¤„ç†å¤±è´¥: {str(e)}\n\n```\n{traceback.format_exc()}\n```"
        return frame, None, error_msg

def process_webcam_frame(frame, confidence_threshold, show_depth):
    """å®æ—¶å¤„ç†æ‘„åƒå¤´å¸§ - å®Œæ•´åŠŸèƒ½ç‰ˆæœ¬"""
    if frame is None:
        return None, None, "â³ ç­‰å¾…æ‘„åƒå¤´è¾“å…¥..."
    
    try:
        start_time = time.time()
        pipe = initialize_pipeline()
        
        # ç¡®ä¿å›¾åƒæ ¼å¼æ­£ç¡®
        if isinstance(frame, Image.Image):
            frame = np.array(frame)
        
        if len(frame.shape) == 2:
            frame = cv2.cvtColor(frame, cv2.COLOR_GRAY2RGB)
        elif frame.shape[2] == 4:
            frame = cv2.cvtColor(frame, cv2.COLOR_RGBA2RGB)
        
        # æ‰§è¡Œå®Œæ•´æ£€æµ‹
        result = pipe.process_image(
            frame,
            confidence_threshold=confidence_threshold,
            compute_depth=show_depth,
            compute_distance=True  # å¯ç”¨è·ç¦»æ£€æµ‹
        )
        
        detections = result['detections']
        depth_map = result.get('depth_map')
        
        # ç»˜åˆ¶æ£€æµ‹ç»“æœ
        output_frame = draw_detections(frame.copy(), detections)
        
        # æ·»åŠ æ€§èƒ½ä¿¡æ¯
        process_time = time.time() - start_time
        fps = 1.0 / process_time if process_time > 0 else 0
        cv2.putText(output_frame, f"FPS: {fps:.1f}", (10, 30),
                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        cv2.putText(output_frame, f"Detections: {len(detections)}", (10, 70),
                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
        
        # ç”Ÿæˆç»Ÿè®¡ä¿¡æ¯
        stats = f"""
        ### ğŸ“Š å®æ—¶æ£€æµ‹ç»Ÿè®¡
        - **æ£€æµ‹æ•°é‡**: {len(detections)} ä¸ªç¯å…·
        - **å¤„ç†æ—¶é—´**: {process_time:.2f}ç§’
        - **FPS**: {fps:.2f}
        - **ç½®ä¿¡åº¦é˜ˆå€¼**: {confidence_threshold:.2f}
        
        ### ğŸ” æ£€æµ‹è¯¦æƒ…
        """
        
        for i, det in enumerate(detections[:10], 1):
            stats += f"\n**ç›®æ ‡ {i}**\n"
            stats += f"- ç±»å‹: {det['label']}\n"
            stats += f"- ç½®ä¿¡åº¦: {det['confidence']:.2%}\n"
            if det.get('distance'):
                stats += f"- è·ç¦»: {det['distance']:.2f}m\n"
        
        # ç”Ÿæˆæ·±åº¦å›¾
        depth_image = None
        if show_depth and depth_map is not None:
            import matplotlib.pyplot as plt
            import matplotlib
            matplotlib.use('Agg')
            
            fig, ax = plt.subplots(figsize=(8, 6))
            im = ax.imshow(depth_map, cmap='plasma')
            ax.set_title('Depth Map', fontsize=14)
            ax.axis('off')
            plt.colorbar(im, ax=ax, label='Depth (normalized)')
            
            fig.canvas.draw()
            buf = fig.canvas.buffer_rgba()
            depth_image = np.asarray(buf)[:, :, :3]
            plt.close(fig)
        
        return output_frame, depth_image, stats
        
    except Exception as e:
        import traceback
        error_msg = f"âŒ å¤„ç†å¤±è´¥: {str(e)}\n\n```\n{traceback.format_exc()}\n```"
        return frame, None, error_msg

def process_webcam_continuous(frame, confidence_threshold, show_depth, is_running):
    """è¿ç»­å¤„ç†æ‘„åƒå¤´å¸§ - ç”¨äºè‡ªåŠ¨é—´éš”é‡‡æ ·"""
    if not is_running or frame is None:
        return None, None, "â¸ï¸ æ£€æµ‹å·²åœæ­¢", is_running
    
    # è°ƒç”¨æ ‡å‡†å¤„ç†å‡½æ•°
    output_frame, depth_image, stats = process_webcam_frame(frame, confidence_threshold, show_depth)
    
    return output_frame, depth_image, stats, is_running

# åˆ›å»ºGradioç•Œé¢
with gr.Blocks(title="ç¯å…·3Då®šä½æ£€æµ‹ç³»ç»Ÿ (ä¼˜åŒ–ç‰ˆ)", theme=gr.themes.Soft()) as demo:
    gr.Markdown("""
    # ğŸ”¦ ç¯å…·3Då®šä½æ£€æµ‹ç³»ç»Ÿ (TensorRTä¼˜åŒ–ç‰ˆ)
    
    åŸºäº **OWLv2 + DINOv3 + Depth Anything V2** çš„æ™ºèƒ½ç¯å…·æ£€æµ‹ä¸3Då®šä½
    
    âš¡ **æ€§èƒ½ä¼˜åŒ–**: TensorRTåŠ é€Ÿ | é—´éš”é‡‡æ · | è§†é¢‘æµæ£€æµ‹ | GPUåŠ é€Ÿ
    """)
    
    with gr.Tabs():
        # Tab 1: å›¾åƒæ£€æµ‹ (åˆå¹¶åŸå›¾åƒæ£€æµ‹å’Œé—´éš”æ£€æµ‹)
        with gr.Tab("ğŸ“¸ å›¾åƒæ£€æµ‹"):
            gr.Markdown("""
            ### ğŸ“· å•å¼ å›¾ç‰‡æ£€æµ‹
            
            **åŠŸèƒ½**: ä¸Šä¼ å›¾ç‰‡è¿›è¡Œå®Œæ•´çš„ç¯å…·æ£€æµ‹ã€è·ç¦»ä¼°è®¡å’Œæ·±åº¦åˆ†æ
            
            **é€‚ç”¨åœºæ™¯**: 
            - åˆ†æå•å¼ ç…§ç‰‡
            - éªŒè¯æ£€æµ‹æ•ˆæœ
            - è·å–è¯¦ç»†çš„æ£€æµ‹ç»“æœå’Œè·ç¦»ä¿¡æ¯
            """)
            
            with gr.Row():
                with gr.Column():
                    input_image = gr.Image(label="ä¸Šä¼ å›¾åƒ", type="numpy")
                    confidence_slider = gr.Slider(
                        minimum=0.05,
                        maximum=0.5,
                        value=0.15,
                        step=0.05,
                        label="ç½®ä¿¡åº¦é˜ˆå€¼"
                    )
                    show_depth_check = gr.Checkbox(
                        label="æ˜¾ç¤ºæ·±åº¦å›¾",
                        value=True
                    )
                    detect_btn = gr.Button("ğŸ” å¼€å§‹æ£€æµ‹", variant="primary", size="lg")
                
                with gr.Column():
                    output_image = gr.Image(label="æ£€æµ‹ç»“æœ")
                    depth_image = gr.Image(label="æ·±åº¦å›¾", visible=True)
            
            with gr.Row():
                stats_output = gr.Markdown(label="æ£€æµ‹ç»Ÿè®¡")
            
            detect_btn.click(
                fn=process_image,
                inputs=[input_image, confidence_slider, show_depth_check],
                outputs=[output_image, depth_image, stats_output]
            )
        
        # Tab 2: é—´éš”é‡‡æ ·æ£€æµ‹ (å®šæ—¶é‡å¤æ£€æµ‹ä¸Šä¼ çš„å›¾ç‰‡)
        with gr.Tab("â±ï¸ é—´éš”é‡‡æ ·"):
            gr.Markdown("""
            ### âš¡ é—´éš”é‡‡æ ·ç›‘æ§
            
            **åŠŸèƒ½**: å¯¹ä¸Šä¼ çš„å›¾ç‰‡è¿›è¡Œå®šæ—¶é‡å¤æ£€æµ‹,æ¨¡æ‹Ÿç›‘æ§åœºæ™¯
            
            **ä½¿ç”¨åœºæ™¯**:
            - é•¿æ—¶é—´ç›‘æ§å›ºå®šåœºæ™¯
            - é™ä½GPUè´Ÿè½½(æ¯Nç§’æ£€æµ‹ä¸€æ¬¡)
            - æµ‹è¯•æ£€æµ‹ç¨³å®šæ€§
            
            **ä½¿ç”¨æ–¹æ³•**:
            1. ä¸Šä¼ ä¸€å¼ å›¾ç‰‡
            2. è®¾ç½®æ£€æµ‹é—´éš”(å»ºè®®5-10ç§’)
            3. ç‚¹å‡»"å¼€å§‹é‡‡æ ·"åä¼šè‡ªåŠ¨é‡å¤æ£€æµ‹
            4. ç‚¹å‡»"åœæ­¢"ç»“æŸ
            """)
            
            # æ·»åŠ çŠ¶æ€å˜é‡
            sampling_state = gr.State({"running": False, "count": 0})
            
            with gr.Row():
                with gr.Column():
                    sampling_input = gr.Image(label="ä¸Šä¼ å›¾ç‰‡", type="numpy")
                    sampling_interval = gr.Slider(
                        minimum=3,
                        maximum=30,
                        value=5,
                        step=1,
                        label="æ£€æµ‹é—´éš”(ç§’)"
                    )
                    sampling_confidence = gr.Slider(
                        minimum=0.05,
                        maximum=0.5,
                        value=0.15,
                        step=0.05,
                        label="ç½®ä¿¡åº¦é˜ˆå€¼"
                    )
                    sampling_depth_check = gr.Checkbox(
                        label="æ˜¾ç¤ºæ·±åº¦å›¾",
                        value=False
                    )
                    with gr.Row():
                        sampling_start_btn = gr.Button("â–¶ï¸ å¼€å§‹é‡‡æ ·", variant="primary")
                        sampling_stop_btn = gr.Button("â¹ï¸ åœæ­¢", variant="stop")
                
                with gr.Column():
                    sampling_output = gr.Image(label="æ£€æµ‹ç»“æœ")
                    sampling_depth = gr.Image(label="æ·±åº¦å›¾")
            
            with gr.Row():
                sampling_stats = gr.Markdown(label="é‡‡æ ·ç»Ÿè®¡", value="â¸ï¸ ç­‰å¾…å¼€å§‹...")
            
            # ç‚¹å‡»å¼€å§‹é‡‡æ ·
            sampling_start_btn.click(
                fn=process_frame_interval,
                inputs=[sampling_input, sampling_confidence, sampling_interval, sampling_depth_check],
                outputs=[sampling_output, sampling_depth, sampling_stats],
                every=5  # æ¯5ç§’è§¦å‘ä¸€æ¬¡
            )
            
            sampling_stop_btn.click(
                fn=lambda: "â¹ï¸ å·²åœæ­¢é‡‡æ ·",
                outputs=sampling_stats
            )
        
        # Tab 3: è§†é¢‘æµå®æ—¶æ£€æµ‹
        with gr.Tab("ğŸ“¹ è§†é¢‘æµæ£€æµ‹"):
            gr.Markdown("""
            ### ğŸ¥ å®æ—¶è§†é¢‘æµæ£€æµ‹
            
            **å®Œæ•´åŠŸèƒ½**:
            - âœ… ç¯å…·æ£€æµ‹ + è·ç¦»ä¼°è®¡ + æ·±åº¦å›¾
            - âœ… è§†é¢‘æµæŒç»­æ£€æµ‹ (æ‘„åƒå¤´æŒç»­å¼€å¯)
            - âœ… è‡ªåŠ¨é—´éš”é‡‡æ · (æ¯5ç§’æ£€æµ‹ä¸€æ¬¡)
            - âœ… å®æ—¶è§†é¢‘æµæ£€æµ‹ (æ‘„åƒå¤´æŒç»­å¼€å¯)
            - âœ… è‡ªåŠ¨é—´éš”é‡‡æ · (æ¯5ç§’æ£€æµ‹ä¸€æ¬¡)
            - âœ… å®Œæ•´åŠŸèƒ½: æ£€æµ‹ + è·ç¦» + æ·±åº¦
            - âœ… æ£€æµ‹ç»“æœè‡ªåŠ¨åˆ·æ–°
            
            **ä½¿ç”¨æ–¹æ³•**:
            1. ç‚¹å‡»æ‘„åƒå¤´å›¾æ ‡å¯åŠ¨æœ¬åœ°æ‘„åƒå¤´
            2. æ‘„åƒå¤´å¼€å¯åä¼š**è‡ªåŠ¨æ¯5ç§’æ£€æµ‹ä¸€æ¬¡**
            3. è°ƒæ•´ç½®ä¿¡åº¦é˜ˆå€¼å’Œæ·±åº¦å›¾é€‰é¡¹
            4. æ£€æµ‹ç»“æœä¼šè‡ªåŠ¨åˆ·æ–°æ˜¾ç¤º
            
            **æ€§èƒ½æç¤º**:
            - æ¨ç†æ—¶é—´çº¦1-3ç§’/å¸§
            - è‡ªåŠ¨é—´éš”5ç§’,é¿å…GPUè¿‡è½½
            - å…³é—­æ·±åº¦å›¾å¯æå‡é€Ÿåº¦
            - é€‚åˆé•¿æ—¶é—´å®æ—¶ç›‘æ§
            """)
            
            with gr.Row():
                with gr.Column():
                    webcam_input = gr.Image(
                        label="ğŸ“¹ æœ¬åœ°æ‘„åƒå¤´ (è‡ªåŠ¨æ¯5ç§’æ£€æµ‹)",
                        sources=["webcam"],
                        type="numpy",
                        streaming=True  # æŒç»­æµå¼ä¼ è¾“
                    )
                    with gr.Row():
                        webcam_confidence = gr.Slider(
                            minimum=0.05,
                            maximum=0.5,
                            value=0.15,
                            step=0.05,
                            label="ç½®ä¿¡åº¦é˜ˆå€¼"
                        )
                        webcam_depth_check = gr.Checkbox(
                            label="æ˜¾ç¤ºæ·±åº¦å›¾",
                            value=False  # é»˜è®¤å…³é—­æå‡é€Ÿåº¦
                        )
                
                with gr.Column():
                    webcam_output = gr.Image(label="æ£€æµ‹ç»“æœ")
                    webcam_depth = gr.Image(label="æ·±åº¦å›¾")
            
            with gr.Row():
                webcam_stats = gr.Markdown(
                    label="å®æ—¶ç»Ÿè®¡", 
                    value="ğŸ“¹ **å¯åŠ¨æ‘„åƒå¤´åä¼šè‡ªåŠ¨æ¯5ç§’æ£€æµ‹ä¸€æ¬¡...**"
                )
            
            # ä½¿ç”¨ stream å®ç°è§†é¢‘æµè‡ªåŠ¨æ£€æµ‹
            webcam_input.stream(
                fn=process_webcam_frame,
                inputs=[webcam_input, webcam_confidence, webcam_depth_check],
                outputs=[webcam_output, webcam_depth, webcam_stats],
                stream_every=5  # æ¯5ç§’å¤„ç†ä¸€æ¬¡
            )
        
        # Tab 4: ä½¿ç”¨è¯´æ˜
        with gr.Tab("ğŸ“– ä½¿ç”¨æŒ‡å—"):
            gr.Markdown("""
            # ğŸ“– ä½¿ç”¨æŒ‡å—
            
            ## ğŸ“‹ åŠŸèƒ½æ¦‚è§ˆ
            
            ### 1ï¸âƒ£ å›¾åƒæ£€æµ‹
            - **ç”¨é€”**: ä¸Šä¼ å•å¼ å›¾ç‰‡è¿›è¡Œæ£€æµ‹
            - **åŠŸèƒ½**: å®Œæ•´çš„æ£€æµ‹ + è·ç¦»ä¼°è®¡ + æ·±åº¦å›¾
            - **é€‚ç”¨åœºæ™¯**: åˆ†æé™æ€ç…§ç‰‡,éªŒè¯æ£€æµ‹æ•ˆæœ
            - **æ“ä½œ**: ä¸Šä¼ å›¾ç‰‡ â†’ è°ƒæ•´å‚æ•° â†’ ç‚¹å‡»æ£€æµ‹
            
            ### 2ï¸âƒ£ é—´éš”é‡‡æ ·
            - **ç”¨é€”**: å¯¹ä¸Šä¼ çš„å›¾ç‰‡å®šæ—¶é‡å¤æ£€æµ‹
            - **åŠŸèƒ½**: æ¨¡æ‹Ÿç›‘æ§åœºæ™¯,é™ä½GPUè´Ÿè½½
            - **é€‚ç”¨åœºæ™¯**: é•¿æ—¶é—´ç›‘æ§å›ºå®šç”»é¢
            - **æ“ä½œ**: ä¸Šä¼ å›¾ç‰‡ â†’ è®¾ç½®é—´éš” â†’ å¼€å§‹é‡‡æ · â†’ è‡ªåŠ¨é‡å¤æ£€æµ‹
            - **ä¼˜åŠ¿**: GPUè´Ÿè½½é™ä½90%+,é€‚åˆ24å°æ—¶è¿è¡Œ
            
            ### 3ï¸âƒ£ è§†é¢‘æµæ£€æµ‹
            - **ç”¨é€”**: æœ¬åœ°æ‘„åƒå¤´å®æ—¶æ£€æµ‹
            - **åŠŸèƒ½**: è§†é¢‘æµæŒç»­æ£€æµ‹,è‡ªåŠ¨é—´éš”é‡‡æ ·
            - **é€‚ç”¨åœºæ™¯**: å®æ—¶ç›‘æ§,æ¼”ç¤ºå±•ç¤º
            - **æ“ä½œ**: å¯åŠ¨æ‘„åƒå¤´ â†’ è‡ªåŠ¨æ¯5ç§’æ£€æµ‹ä¸€æ¬¡
            - **ä¼˜åŠ¿**: çœŸæ­£çš„å®æ—¶æ£€æµ‹,ç»“æœè‡ªåŠ¨åˆ·æ–°
            - **åŠŸèƒ½**: å®Œæ•´çš„æ£€æµ‹ + è·ç¦»ä¼°è®¡ + æ·±åº¦å›¾
            - **é€‚ç”¨åœºæ™¯**: é•¿æ—¶é—´ç›‘æ§,é™ä½GPUè´Ÿè½½
            - **æ¨èé—´éš”**: 10ç§’
            
            ### 3ï¸âƒ£ å®æ—¶æ£€æµ‹
            - **ç”¨é€”**: ä½¿ç”¨æœ¬åœ°æ‘„åƒå¤´å®æ—¶æ£€æµ‹
            - **åŠŸèƒ½**: å®Œæ•´çš„æ£€æµ‹ + è·ç¦»ä¼°è®¡ + æ·±åº¦å›¾
            - **é€‚ç”¨åœºæ™¯**: å®æ—¶ç›‘æ§å’Œæ¼”ç¤º
            - **æ€§èƒ½æç¤º**: æ¨ç†æ—¶é—´çº¦1-3ç§’/å¸§
            
            ---
            
            ## é«˜çº§ç”¨æˆ·æ–¹æ¡ˆ
            
            ### æ–¹æ¡ˆ1: ä½¿ç”¨ webcam_client.html
            
            **æ­¥éª¤**:
            1. æ‰“å¼€é¡¹ç›®ä¸­çš„ `webcam_client.html` æ–‡ä»¶
            2. è¾“å…¥æœåŠ¡å™¨åœ°å€: `http://æœåŠ¡å™¨IP:7860`
            3. ç‚¹å‡»å¯åŠ¨æ‘„åƒå¤´
            4. è®¾ç½®é‡‡æ ·é—´éš”(å»ºè®®10ç§’)
            
            **ä¼˜åŠ¿**:
            - æœ¬åœ°æ‘„åƒå¤´æ•è·(æ— å»¶è¿Ÿ)
            - é—´éš”å‘é€åˆ°æœåŠ¡å™¨å¤„ç†
            - å®æ—¶æ˜¾ç¤ºFPSå’Œç»Ÿè®¡ä¿¡æ¯
            
            ### æ–¹æ¡ˆ2: Pythonè„šæœ¬
            
            ```python
            # custom_detection.py
            import cv2
            from pipeline import LightLocalization3D
            
            # åˆå§‹åŒ–
            pipeline = LightLocalization3D(...)
            cap = cv2.VideoCapture(0)
            
            while True:
                ret, frame = cap.read()
                if not ret:
                    break
                
                # æ£€æµ‹
                result = pipeline.process_image(
                    frame,
                    confidence_threshold=0.15,
                    compute_depth=True,
                    compute_distance=True
                )
                
                # æ˜¾ç¤ºç»“æœ
                for det in result['detections']:
                    print(f"{det['label']}: {det['distance']:.2f}m")
                
                cv2.imshow('Detection', frame)
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    break
            ```
            
            ---
            
            ## TensorRTåŠ é€Ÿ
            
            ç³»ç»Ÿä¼šè‡ªåŠ¨å°è¯•å¯ç”¨TensorRTåŠ é€Ÿã€‚æŸ¥çœ‹å¯åŠ¨æ—¥å¿—:
            - âœ… TensorRTåŠ é€Ÿå·²å¯ç”¨ â†’ é€Ÿåº¦æå‡2-3å€
            - âš ï¸ TensorRTåŠ é€Ÿå¯ç”¨å¤±è´¥ â†’ ä½¿ç”¨æ ‡å‡†PyTorchæ¨ç†
            
            ### å®‰è£…TensorRT (å¯é€‰)
            
            ```bash
            pip install torch-tensorrt --extra-index-url https://download.pytorch.org/whl/cu121
            ```
            
            ---
            
            ## æ€§èƒ½å¯¹æ¯”
            
            | æ¨¡å¼ | æ¨ç†æ—¶é—´ | GPUè´Ÿè½½ | é€‚ç”¨åœºæ™¯ |
            |------|---------|---------|---------|
            | å›¾åƒæ£€æµ‹ | 1-3ç§’ | 100% | å•å¼ å›¾ç‰‡åˆ†æ |
            | é—´éš”æ£€æµ‹(10s) | 1-3ç§’ | ~10% | é•¿æ—¶é—´ç›‘æ§ |
            | å®æ—¶æ£€æµ‹ | 1-3ç§’ | 100% | å®æ—¶æ¼”ç¤º |
            | TensorRTåŠ é€Ÿ | 0.5-1ç§’ | 100% | é«˜æ€§èƒ½éœ€æ±‚ |
            
            ---
            
            **GitHub**: [é¡¹ç›®åœ°å€](https://github.com/warchanged/Luminaire-Testing-and-Monocular-Depth-Distance)
            """)
        
        # Tab 4: ç³»ç»Ÿä¿¡æ¯
        with gr.Tab("â„¹ï¸ ç³»ç»Ÿä¿¡æ¯"):
            gr.Markdown("""
            ### æŠ€æœ¯æ¶æ„
            
            | ç»„ä»¶ | æ¨¡å‹ | ç”¨é€” |
            |------|------|------|
            | **æ£€æµ‹å™¨** | OWLv2-Large | é›¶æ ·æœ¬ç›®æ ‡æ£€æµ‹ |
            | **ç‰¹å¾æå–** | DINOv3-Large | è‡ªç›‘ç£è§†è§‰ç‰¹å¾ |
            | **æ·±åº¦ä¼°è®¡** | Depth Anything V2 | å•ç›®æ·±åº¦ä¼°è®¡ |
            | **åŠ é€Ÿ** | TensorRT + CUDA | æ¨ç†åŠ é€Ÿ |
            
            ### æ€§èƒ½ä¼˜åŒ–
            
            - âš¡ **TensorRTåŠ é€Ÿ**: 2-3å€é€Ÿåº¦æå‡
            - ğŸ”„ **é—´éš”é‡‡æ ·**: é™ä½90%+ GPUè´Ÿè½½
            - ğŸ“‰ **FP16æ¨ç†**: é™ä½æ˜¾å­˜å ç”¨
            - ğŸ¯ **æŒ‰éœ€è®¡ç®—**: ä»…åœ¨éœ€è¦æ—¶è®¡ç®—æ·±åº¦
            
            ### æ”¯æŒçš„ç¯å…·ç±»å‹
            
            - ğŸ® åŠç¯ç±»: chandelier, pendant light, hanging lamp
            - ğŸ’¡ å¸é¡¶ç¯: ceiling light, flush mount, recessed light
            - ğŸ•¯ï¸ å£ç¯: wall lamp, wall sconce
            - ğŸ›‹ï¸ å°ç¯: table lamp, desk lamp
            - ğŸŒŸ å°„ç¯: spotlight, track light
            - âœ¨ LEDç¯: LED panel, LED strip
            - ... å…±33ç§ç±»å‹
            
            ### GitHub
            
            [ğŸ”— é¡¹ç›®åœ°å€](https://github.com/warchanged/Luminaire-Testing-and-Monocular-Depth-Distance)
            """)

if __name__ == "__main__":
    demo.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=False,
        show_error=True,
        allowed_paths=["/"],
        root_path=None
    )
